{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename,mode='rt',encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now clean each sentence: \n",
    "\n",
    "    Remove all non-printable characters.\n",
    "    Remove all punctuation characters.\n",
    "    Normalize all Unicode characters to ASCII (e.g. Latin characters).\n",
    "    Normalize the case to lowercase.\n",
    "    Remove any remaining tokens that are not alphabetic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    #using regex for character filtering\n",
    "    re_print = re.compile('[^%s]' %re.escape(string.printable))\n",
    "    \n",
    "    #prepare translation table for removing punctuation\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    \n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            #normalize unicode char\n",
    "            line = normalize('NFD',line).encode('ascii','ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            #tokenize on white space\n",
    "            line = line.split()\n",
    "            #now convert it to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            #remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            #remove non-printable characters from each line\n",
    "            line = [re_print.sub('',w) for w in line]\n",
    "            #remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            \n",
    "            #store as string\n",
    "            clean_pair.append(''.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the cleaned data to a file\n",
    "def save_cleaned_data(sentences,filename):\n",
    "    dump(sentences,open(filename,'wb'))\n",
    "    print('Saved: %s' %filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n"
     ]
    }
   ],
   "source": [
    "#calling functions\n",
    "#load dataset\n",
    "filename = 'deu.txt'\n",
    "doc = load_doc(filename)\n",
    "#split it into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "#clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "#save it into a file\n",
    "save_cleaned_data(clean_pairs,'english-german.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152820, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pairs.shape  #152,820 words #2 columns (1 for each lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'hallo'],\n",
       "       ['hi', 'grugott'],\n",
       "       ['run', 'lauf'],\n",
       "       ...,\n",
       "       ['ifsomeonewhodoesntknowyourbackgroundsaysthatyousoundlikeanativespeakeritmeanstheyprobablynoticedsomethingaboutyourspeakingthatmadethemrealizeyouwerentanativespeakerinotherwordsyoudontreallysoundlikeanativespeaker',\n",
       "        'wennjemandderdeineherkunftnichtkenntsagtdassduwieeinmuttersprachlersprichstbedeutetdasdassmanwahrscheinlichetwasandeinersprechweisebemerkthatdaserkennenliedassdukeinmuttersprachlerbistmitanderenwortenduhorstdichnichtwirklichwieeinmuttersprachleran'],\n",
       "       ['ifsomeonewhodoesntknowyourbackgroundsaysthatyousoundlikeanativespeakeritmeanstheyprobablynoticedsomethingaboutyourspeakingthatmadethemrealizeyouwerentanativespeakerinotherwordsyoudontreallysoundlikeanativespeaker',\n",
       "        'wennjemandfremdesdirsagtdassdudichwieeinmuttersprachleranhorstbedeutetdaswahrscheinlicherhatetwasandeinemsprechenbemerktdassdichalsnichtmuttersprachlerverratenhatmitanderenwortenduhorstdichnichtwirklichwieeinmuttersprachleran'],\n",
       "       ['itmaybeimpossibletogetacompletelyerrorfreecorpusduetothenatureofthiskindofcollaborativeefforthoweverifweencouragememberstocontributesentencesintheirownlanguagesratherthanexperimentinlanguagestheyarelearningwemightbeabletominimizeerrors',\n",
       "        'esistwohlunmoglicheinenvollkommenfehlerfreienkorpuszuerreichendasliegtindernatureinessolchengemeinschaftsprojektsdochwennwirunseremitgliederdazubringenkonnennichtmitsprachenherumzuexperimentierendiesiegeradelernensondernsatzeinihrereigenenmuttersprachebeizutragendanngelingtesunsvielleichtdiezahlderfehlerkleinzuhalten']],\n",
       "      dtype='<U318')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hi] ==> [hallo]\n",
      "[hi] ==> [grugott]\n",
      "[run] ==> [lauf]\n",
      "[wow] ==> [potzdonner]\n",
      "[wow] ==> [donnerwetter]\n",
      "[fire] ==> [feuer]\n",
      "[help] ==> [hilfe]\n",
      "[help] ==> [zuhulf]\n",
      "[stop] ==> [stopp]\n",
      "[wait] ==> [warte]\n",
      "[hello] ==> [hallo]\n",
      "[itry] ==> [ichprobierees]\n",
      "[iwon] ==> [ichhabgewonnen]\n",
      "[iwon] ==> [ichhabegewonnen]\n",
      "[smile] ==> [lacheln]\n",
      "[cheers] ==> [zumwohl]\n",
      "[freeze] ==> [keinebewegung]\n",
      "[freeze] ==> [stehenbleiben]\n",
      "[gotit] ==> [verstanden]\n",
      "[gotit] ==> [einverstanden]\n",
      "[heran] ==> [errannte]\n",
      "[heran] ==> [erlief]\n",
      "[hopin] ==> [machmit]\n",
      "[hugme] ==> [druckmich]\n",
      "[hugme] ==> [nimmmichindenarm]\n",
      "[hugme] ==> [umarmemich]\n",
      "[ifell] ==> [ichfiel]\n",
      "[ifell] ==> [ichfielhin]\n",
      "[ifell] ==> [ichsturzte]\n",
      "[ifell] ==> [ichbinhingefallen]\n",
      "[ifell] ==> [ichbingesturzt]\n",
      "[iknow] ==> [ichwei]\n",
      "[ilied] ==> [ichhabegelogen]\n",
      "[ilost] ==> [ichhabeverloren]\n",
      "[im] ==> [ichbinjahrealt]\n",
      "[im] ==> [ichbin]\n",
      "[imok] ==> [mirgehtsgut]\n",
      "[imok] ==> [esgehtmirgut]\n",
      "[noway] ==> [unmoglich]\n",
      "[noway] ==> [dasgibtsdochnicht]\n",
      "[noway] ==> [ausgeschlossen]\n",
      "[noway] ==> [inkeinsterweise]\n",
      "[really] ==> [wirklich]\n",
      "[really] ==> [echt]\n",
      "[really] ==> [imernst]\n",
      "[thanks] ==> [danke]\n",
      "[tryit] ==> [versuchs]\n",
      "[whyme] ==> [warumich]\n",
      "[asktom] ==> [fragtom]\n",
      "[asktom] ==> [fragensietom]\n",
      "[asktom] ==> [fragttom]\n",
      "[becool] ==> [entspanndich]\n",
      "[befair] ==> [seinichtungerecht]\n",
      "[befair] ==> [seifair]\n",
      "[benice] ==> [seinett]\n",
      "[benice] ==> [seiensienett]\n",
      "[beatit] ==> [gehweg]\n",
      "[beatit] ==> [hauab]\n",
      "[beatit] ==> [verschwinde]\n",
      "[beatit] ==> [verdufte]\n",
      "[beatit] ==> [machdichfort]\n",
      "[beatit] ==> [ziehleine]\n",
      "[beatit] ==> [machdichvomacker]\n",
      "[beatit] ==> [verziehdich]\n",
      "[beatit] ==> [verkrumeledich]\n",
      "[beatit] ==> [trolldich]\n",
      "[beatit] ==> [zischab]\n",
      "[beatit] ==> [packdich]\n",
      "[beatit] ==> [machnefliege]\n",
      "[beatit] ==> [schwirrab]\n",
      "[beatit] ==> [machdiesause]\n",
      "[beatit] ==> [scherdichweg]\n",
      "[beatit] ==> [scherdichfort]\n",
      "[callme] ==> [rufmichan]\n",
      "[comein] ==> [kommherein]\n",
      "[comein] ==> [herein]\n",
      "[comeon] ==> [komm]\n",
      "[comeon] ==> [kommt]\n",
      "[comeon] ==> [machschon]\n",
      "[comeon] ==> [machtschon]\n",
      "[getout] ==> [raus]\n",
      "[goaway] ==> [gehweg]\n",
      "[goaway] ==> [hauab]\n",
      "[goaway] ==> [verschwinde]\n",
      "[goaway] ==> [verdufte]\n",
      "[goaway] ==> [machdichfort]\n",
      "[goaway] ==> [ziehleine]\n",
      "[goaway] ==> [machdichvomacker]\n",
      "[goaway] ==> [verziehdich]\n",
      "[goaway] ==> [verkrumeledich]\n",
      "[goaway] ==> [trolldich]\n",
      "[goaway] ==> [zischab]\n",
      "[goaway] ==> [packdich]\n",
      "[goaway] ==> [machnefliege]\n",
      "[goaway] ==> [schwirrab]\n",
      "[goaway] ==> [machdiesause]\n",
      "[goaway] ==> [scherdichweg]\n",
      "[goaway] ==> [scherdichfort]\n",
      "[goaway] ==> [gehweg]\n",
      "[goaway] ==> [verpissdich]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print('[%s] ==> [%s]' %(clean_pairs[i][0],clean_pairs[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the dataset to 10k examples \n",
    "9k for training and 1k for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load,dump\n",
    "from numpy.random import rand,shuffle\n",
    "\n",
    "#load the clean dataset\n",
    "def load_clean_dataset(filename):\n",
    "    return load(open(filename,'rb'))\n",
    "\n",
    "#for saving a list of clean sentences to file\n",
    "def save_clean_data(sentences,filename):\n",
    "    dump(sentences,open(filename,'wb'))\n",
    "    print('Saved: %s' %filename)\n",
    "    \n",
    "raw_dataset = load_clean_dataset('english-german.pkl')\n",
    "\n",
    "#reducing dataset size to 10k\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences,:]\n",
    "\n",
    "#random shuffle\n",
    "shuffle(dataset)\n",
    "\n",
    "#split it into train(9k) and test(1k) data\n",
    "train,test = dataset[:9000],dataset[9000:]\n",
    "\n",
    "#save it \n",
    "save_clean_data(dataset,'english-german-both.pkl')\n",
    "save_clean_data(train,'english-german-train.pkl')\n",
    "save_clean_data(test,'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
